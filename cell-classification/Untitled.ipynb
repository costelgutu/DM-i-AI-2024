{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca570194-f310-43b6-8b1c-95df1751d9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 19:33:29.099082: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-01 19:33:29.661879: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-01 19:33:29.737991: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-01 19:33:30.430536: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-01 19:33:33.522732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.client import device_lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7df90c4-256f-499d-8680-b1e7d78bfa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow is using the following GPU(s):\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# Check if TensorFlow is using a GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"TensorFlow is using the following GPU(s):\")\n",
    "    for gpu in gpus:\n",
    "        print(gpu)\n",
    "else:\n",
    "    print(\"No GPU detected for TensorFlow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99875a2a-3131-4977-a1ed-985e26dbbfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a0559d-4b15-43e0-939d-714d807301aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 19:35:12.016575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78852 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:41:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU computation time: 4.285232782363892\n",
      "GPU used: /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Create a random tensor\n",
    "matrix1 = tf.random.normal([10000, 10000])\n",
    "matrix2 = tf.random.normal([10000, 10000])\n",
    "\n",
    "# Perform matrix multiplication on the GPU\n",
    "start_time = time.time()\n",
    "result = tf.matmul(matrix1, matrix2)\n",
    "print(\"GPU computation time:\", time.time() - start_time)\n",
    "\n",
    "# Check if the operation was performed on the GPU\n",
    "print(\"GPU used:\", result.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bce90bc6-4fa8-4b8e-99a3-75d033f6260e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80227a18-f630-4cc7-9ff3-fa581f2c6676",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Allow TensorFlow to only use a limited amount of memory on the GPU\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0fc65cc-388d-4923-bde6-af8957f852ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:\n",
      "/device:CPU:0 CPU\n",
      "/device:GPU:0 GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 19:36:06.847644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /device:GPU:0 with 78852 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:41:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "# List all available devices\n",
    "devices = device_lib.list_local_devices()\n",
    "print(\"Available devices:\")\n",
    "for device in devices:\n",
    "    print(device.name, device.device_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cac0e5a-e0e3-4c86-a943-97afd53a289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_dir = 'data/training'  # Folder with .tif images\n",
    "csv_file = 'data/training.csv'  # CSV file with image_id and is_homogeneous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "402d6cf7-6078-49ab-9e28-e80a7cf7c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df883fd6-5efb-4821-a7ed-4d05f5fa7b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8596b33b-4655-444a-ae43-0731b38d7bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[279 277 275 273 271 269 267 265 263 261 259 257 255 253 251 249 247 245\n",
      " 243 241 239 237 235 233 231 229 227 225 223 221 219 217 215 213 211 209\n",
      " 207 205 203 201 199 197 195 193 191 189 187 185 183 181 179 177 175 173\n",
      " 171 169 167 165 163 161 159 157 155 153 151 149 147 145 143 141 139 137\n",
      " 135 133 131 129 127 125 123 121 119 117 115 113 111 109 107 105 103 101\n",
      "  99  97  95  93  91  89  87  85  83  81  79  77  75  73  71  69  67  65\n",
      "  63  61  59  57  55  53  51  49  47  45  43  41  39  37  35  33  29  27\n",
      "  25  23  21  19  17  15  13  11   9   7   5   3   1]\n"
     ]
    }
   ],
   "source": [
    "# Strip any leading/trailing spaces from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Now you can access 'image_id' without the extra space\n",
    "image_id_values = df['image_id'].values\n",
    "print(image_id_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7ec9a60-5aea-4456-8c7c-2d50225856db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path):\n",
    "    # Load image with PIL and convert to an array\n",
    "    img = load_img(image_path, target_size=IMG_SIZE)\n",
    "    img_array = img_to_array(img)\n",
    "    # Normalize image pixel values (0-255 -> 0-1)\n",
    "    img_array = img_array / 255.0\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adbab277-acf9-4a4c-9a67-f93eab7dd78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_id', 'is_homogenous'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a7339fa-8efc-4a0b-bdff-c645bfc14842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create lists of image paths and labels\n",
    "# Assuming image_id values need to be 3 digits with leading zeros\n",
    "image_paths = [os.path.join(data_dir, f\"{str(image_id).zfill(3)}.tif\") for image_id in df['image_id']]\n",
    "labels = df['is_homogenous'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f405297a-200b-400d-874c-1c059e0057a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/training/279.tif',\n",
       " 'data/training/277.tif',\n",
       " 'data/training/275.tif',\n",
       " 'data/training/273.tif',\n",
       " 'data/training/271.tif',\n",
       " 'data/training/269.tif',\n",
       " 'data/training/267.tif',\n",
       " 'data/training/265.tif',\n",
       " 'data/training/263.tif',\n",
       " 'data/training/261.tif',\n",
       " 'data/training/259.tif',\n",
       " 'data/training/257.tif',\n",
       " 'data/training/255.tif',\n",
       " 'data/training/253.tif',\n",
       " 'data/training/251.tif',\n",
       " 'data/training/249.tif',\n",
       " 'data/training/247.tif',\n",
       " 'data/training/245.tif',\n",
       " 'data/training/243.tif',\n",
       " 'data/training/241.tif',\n",
       " 'data/training/239.tif',\n",
       " 'data/training/237.tif',\n",
       " 'data/training/235.tif',\n",
       " 'data/training/233.tif',\n",
       " 'data/training/231.tif',\n",
       " 'data/training/229.tif',\n",
       " 'data/training/227.tif',\n",
       " 'data/training/225.tif',\n",
       " 'data/training/223.tif',\n",
       " 'data/training/221.tif',\n",
       " 'data/training/219.tif',\n",
       " 'data/training/217.tif',\n",
       " 'data/training/215.tif',\n",
       " 'data/training/213.tif',\n",
       " 'data/training/211.tif',\n",
       " 'data/training/209.tif',\n",
       " 'data/training/207.tif',\n",
       " 'data/training/205.tif',\n",
       " 'data/training/203.tif',\n",
       " 'data/training/201.tif',\n",
       " 'data/training/199.tif',\n",
       " 'data/training/197.tif',\n",
       " 'data/training/195.tif',\n",
       " 'data/training/193.tif',\n",
       " 'data/training/191.tif',\n",
       " 'data/training/189.tif',\n",
       " 'data/training/187.tif',\n",
       " 'data/training/185.tif',\n",
       " 'data/training/183.tif',\n",
       " 'data/training/181.tif',\n",
       " 'data/training/179.tif',\n",
       " 'data/training/177.tif',\n",
       " 'data/training/175.tif',\n",
       " 'data/training/173.tif',\n",
       " 'data/training/171.tif',\n",
       " 'data/training/169.tif',\n",
       " 'data/training/167.tif',\n",
       " 'data/training/165.tif',\n",
       " 'data/training/163.tif',\n",
       " 'data/training/161.tif',\n",
       " 'data/training/159.tif',\n",
       " 'data/training/157.tif',\n",
       " 'data/training/155.tif',\n",
       " 'data/training/153.tif',\n",
       " 'data/training/151.tif',\n",
       " 'data/training/149.tif',\n",
       " 'data/training/147.tif',\n",
       " 'data/training/145.tif',\n",
       " 'data/training/143.tif',\n",
       " 'data/training/141.tif',\n",
       " 'data/training/139.tif',\n",
       " 'data/training/137.tif',\n",
       " 'data/training/135.tif',\n",
       " 'data/training/133.tif',\n",
       " 'data/training/131.tif',\n",
       " 'data/training/129.tif',\n",
       " 'data/training/127.tif',\n",
       " 'data/training/125.tif',\n",
       " 'data/training/123.tif',\n",
       " 'data/training/121.tif',\n",
       " 'data/training/119.tif',\n",
       " 'data/training/117.tif',\n",
       " 'data/training/115.tif',\n",
       " 'data/training/113.tif',\n",
       " 'data/training/111.tif',\n",
       " 'data/training/109.tif',\n",
       " 'data/training/107.tif',\n",
       " 'data/training/105.tif',\n",
       " 'data/training/103.tif',\n",
       " 'data/training/101.tif',\n",
       " 'data/training/099.tif',\n",
       " 'data/training/097.tif',\n",
       " 'data/training/095.tif',\n",
       " 'data/training/093.tif',\n",
       " 'data/training/091.tif',\n",
       " 'data/training/089.tif',\n",
       " 'data/training/087.tif',\n",
       " 'data/training/085.tif',\n",
       " 'data/training/083.tif',\n",
       " 'data/training/081.tif',\n",
       " 'data/training/079.tif',\n",
       " 'data/training/077.tif',\n",
       " 'data/training/075.tif',\n",
       " 'data/training/073.tif',\n",
       " 'data/training/071.tif',\n",
       " 'data/training/069.tif',\n",
       " 'data/training/067.tif',\n",
       " 'data/training/065.tif',\n",
       " 'data/training/063.tif',\n",
       " 'data/training/061.tif',\n",
       " 'data/training/059.tif',\n",
       " 'data/training/057.tif',\n",
       " 'data/training/055.tif',\n",
       " 'data/training/053.tif',\n",
       " 'data/training/051.tif',\n",
       " 'data/training/049.tif',\n",
       " 'data/training/047.tif',\n",
       " 'data/training/045.tif',\n",
       " 'data/training/043.tif',\n",
       " 'data/training/041.tif',\n",
       " 'data/training/039.tif',\n",
       " 'data/training/037.tif',\n",
       " 'data/training/035.tif',\n",
       " 'data/training/033.tif',\n",
       " 'data/training/029.tif',\n",
       " 'data/training/027.tif',\n",
       " 'data/training/025.tif',\n",
       " 'data/training/023.tif',\n",
       " 'data/training/021.tif',\n",
       " 'data/training/019.tif',\n",
       " 'data/training/017.tif',\n",
       " 'data/training/015.tif',\n",
       " 'data/training/013.tif',\n",
       " 'data/training/011.tif',\n",
       " 'data/training/009.tif',\n",
       " 'data/training/007.tif',\n",
       " 'data/training/005.tif',\n",
       " 'data/training/003.tif',\n",
       " 'data/training/001.tif']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c11d285-2fff-445b-8266-906f0bc7d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load images and preprocess them\n",
    "images = np.array([load_and_preprocess_image(image_path) for image_path in image_paths])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "368c5bcb-3092-4271-a8e6-7603ae9b187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Split the data into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e709934-2ec9-4b40-a7c6-8297c300f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create data generators for augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d44a5de5-e2e3-4842-b354-6373d230b8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2f261b8-b327-47da-a0c0-40973a84c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Create the data generators\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b7a4995-4cd5-4d60-a905-f9f6c7ef25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Load the pre-trained VGG16 model without the top layer\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5896eac9-9d3f-4528-b9c7-b82253c3632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21af63be-f3c2-40a2-a036-f576e4919ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Create the model by adding custom layers on top of the pre-trained base model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Add dropout for regularization\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b31098a8-4233-430e-89b4-6c7781835594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bacecd4-86ca-4c88-b998-6f4ccdcd8913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cogu/.conda/envs/cellclass/lib/python3.9/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727804174.881365   34976 service.cc:146] XLA service 0x7f3dc800be90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1727804174.881409   34976 service.cc:154]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0\n",
      "2024-10-01 19:36:14.917308: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-01 19:36:16.376219: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2024-10-01 19:36:16.496223: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:762] The NVIDIA driver's CUDA version is 12.0 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "I0000 00:00:1727804192.489708   34976 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 3s/step - accuracy: 0.3430 - loss: 0.9428 - val_accuracy: 0.1429 - val_loss: 0.8891\n",
      "Epoch 2/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.3124 - loss: 0.9086 - val_accuracy: 0.1429 - val_loss: 0.7784\n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.4836 - loss: 0.8145 - val_accuracy: 0.8571 - val_loss: 0.6902\n",
      "Epoch 4/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5785 - loss: 0.6507 - val_accuracy: 0.8571 - val_loss: 0.6198\n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.6092 - loss: 0.6490 - val_accuracy: 0.8571 - val_loss: 0.5652\n",
      "Epoch 6/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6730 - loss: 0.6302 - val_accuracy: 0.8571 - val_loss: 0.5254\n",
      "Epoch 7/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7230 - loss: 0.6297 - val_accuracy: 0.8571 - val_loss: 0.4960\n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.7814 - loss: 0.5109 - val_accuracy: 0.8571 - val_loss: 0.4719\n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8503 - loss: 0.4918 - val_accuracy: 0.8571 - val_loss: 0.4559\n",
      "Epoch 10/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7970 - loss: 0.5090 - val_accuracy: 0.8571 - val_loss: 0.4444\n"
     ]
    }
   ],
   "source": [
    "# 12. Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,  # Adjust the number of epochs as needed\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "842b1639-05b0-4847-ae7f-471dae59061d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8423 - loss: 0.4608\n",
      "Validation Accuracy: 85.71%\n"
     ]
    }
   ],
   "source": [
    "# 13. Evaluate the model on the validation set\n",
    "val_loss, val_acc = model.evaluate(val_generator)\n",
    "print(f\"Validation Accuracy: {val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8baff1d5-23db-4c1f-aab2-c7f26aee1ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('vgg16_homogeneous_classification.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbedb3ba-337b-43e8-ad7c-04a9dce65aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cellclass)",
   "language": "python",
   "name": "cellclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
